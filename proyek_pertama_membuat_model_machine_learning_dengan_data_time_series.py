# -*- coding: utf-8 -*-
"""Proyek_Pertama_Membuat Model Machine Learning dengan Data Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ds-HAsuvlXhqm1xZFX2X0Eu3b91OEdaO
"""

# Upload Dataset File
from google.colab import files
uploaded = files.upload()

# Import Library are required
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

data_train = pd.read_csv('IOT-temp-1.csv')
df_data_train = data_train.drop(columns=['id','room_id/id'])

df_data_train.tail()

# Checking missing file
data_train.isnull().sum()

# Separate the atribute and labels
dates = data_train['noted_date'].values
temp  = data_train['temp'].values

plt.figure(figsize=(15,5))
plt.plot(dates, temp)
plt.title('IOT temperature Forecasting',
          fontsize=20);

# Split the data using train_test_split
from sklearn.model_selection import train_test_split
temp_latih, temp_val = train_test_split(temp, test_size=0.2, shuffle=False)

# Change the data format that model can be used.
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

# Making Layer
train_set = windowed_dataset(temp_latih, window_size=100, batch_size=100, shuffle_buffer=1000)
test_set = windowed_dataset(temp_val, window_size=100, batch_size=100, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True, input_shape=[None,1]),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(16, activation="relu"),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1),
])

# Calculate Minimum MAE Value
minMAE = (data_train['temp'].max() - data_train['temp'].min()) * 0.1
minMAE

# Make CallBack Function
class TestCallback(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
        if(logs.get('mae') < minMAE):
            print("\nmae < 10%!") 
            self.model.stop_training = True 
callbacks = TestCallback()

#Train Model 
optimizer = tf.keras.optimizers.SGD(lr=1.0000e-01, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer='Adam',
              metrics=["mae"])
history = model.fit(train_set, validation_data=test_set, epochs=2000, callbacks = [callbacks])

# Creating accuracy plots for CNN models
plt.figure(figsize=(10,4))
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('model mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.grid(True)
plt.show()
print()

# Creating loss plots for CNN models
plt.figure(figsize=(10,4))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.grid(True)
plt.show()